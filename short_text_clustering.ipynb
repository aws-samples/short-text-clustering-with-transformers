{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "// SPDX-License-Identifier: MIT-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a short text clustering system using AWS SageMaker jumpstart pre-trained transformer models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Setup](#Setup)\n",
    "3. [Create a model for text embeddings from the Jumpstart solutions library of models](#Create-a-model-for-text-embeddings-from-the-Jumpstart-solutions-library-of-models)\n",
    "4. [Data pre-processing](#Data-pre-processing)\n",
    "5. [Create phrase (sentence) embeddings](#Create-phrase-(sentence)-embeddings)\n",
    "6. [Cluster phrases (sentences)](#Cluster-phrases-(sentences))\n",
    "7. [Automatic cluster labeling](#Automatic-cluster-labeling)\n",
    "8. [Batch process the entire dataset](#Batch-process-the-entire-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how you can cluster short text (phrases) using the pre-trained transformer models on [AWS SageMaker Jumpstart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html). Here we will demonstrate the use of a transformer model called [bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli). The model is used to create an embedding of phrases that we will then use to cluster such phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by updating the required packages i.e. SageMaker Python SDK, pandas, numpy, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext-wheel wikipedia boto3 jsonlines seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Note: Restart the notebook's kernel after installing the above packages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use NLTK library to help us with the pre-processing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "sagemaker_execution_role = get_execution_role()\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model for text embeddings from the Jumpstart solutions library of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use one the text embedding [models available in SageMaker jumpstart](https://sagemaker.readthedocs.io/en/v2.129.0/doc_utils/pretrainedmodels.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Chose a model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We choose the tensorflow-tcembedding-universal-sentence-encoder-cmlm-en-large-1 as the default model since it is better suited for phrase analysis\n",
    "\n",
    "model_id, model_version = (\n",
    "    \"tensorflow-tcembedding-universal-sentence-encoder-cmlm-en-large-1\", \n",
    "    \"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can continue with the default model, or can choose a different model from the dropdown generated upon running the next cell. A complete list of SageMaker pre-trained models can also be accessed at [Sagemaker pre-trained Models](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models, list_jumpstart_tasks\n",
    "\n",
    "# Retrieves all text embedding models.\n",
    "filter_value = \"task == tcembedding\"\n",
    "tcembedding_models = list_jumpstart_models(filter=filter_value)\n",
    "\n",
    "# display the model-ids in a dropdown to select a model for inference.\n",
    "model_dropdown = Dropdown(\n",
    "    options=tcembedding_models,\n",
    "    value=model_id,\n",
    "    description=\"Select a model\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_version=\"*\" fetches the latest version of the model\n",
    "model_id, model_version = model_dropdown.value, \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = name_from_base(f\"jumpstart-example-infer-{model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model from the selected model_id, model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "inference_instance_type = \"ml.m5.xlarge\"  #You can change the instance according to your needs\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base Tensorflow container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the inference script uri. This includes all dependencies and scripts for model loading, inference handling etc.\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the model uri. This includes the model and model parameters.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create the SageMaker model instance\n",
    "embedding_model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    model_data=model_uri,\n",
    "    entry_point=\"inference.py\",  # entry point file in source_dir and present in deploy_source_uri\n",
    "    role=sagemaker_execution_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demonstration we will use a dataset made up of the blog titles for each blog published by AWS from 2004 until late 2022. We use the blog titles to cluster them and assign a topic to each cluster\n",
    "\n",
    "The text is pre-processed with the following steps:\n",
    "\n",
    "* Set category string to lowercase\n",
    "* Replace acronyms with actual words -> the SBW corpus is less likely to have the acronyms in it than the actual words in relation to each other\n",
    "* Replace special word-bound characters such as / and - (i.e.: imagenes/videos, cerveza-vino) to get separate words.\n",
    "* Eliminate explanations between parenthesis\n",
    "* Remove any other non-word characters from sentence\n",
    "* Split sentence into tokens\n",
    "* Singularize each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_acronyms(sentence):\n",
    "    for rule in ac:\n",
    "        match = re.search(rule[0], sentence, re.IGNORECASE)\n",
    "        if match:\n",
    "            sentence = re.sub(rule[0], rule[1], sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs_df = pd.read_csv('aws_blog_titles.csv', header=None, names=['URL', 'Title'])\n",
    "\n",
    "aws_acronyms_df = pd.read_csv('acronyms.csv', header=None, delimiter=';', names=['acronym', 'meaning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blogs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We transform acronyms to their actual meaning since the transformer may not be aware of them (as it was not trained in this specific vocabulary)\n",
    "\n",
    "aws_acronyms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs_df = blogs_df.drop_duplicates(subset=['Title']).reset_index()\n",
    "blogs_df = blogs_df.drop(columns=['index', 'URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For eficiency only take sample_size titles at random\n",
    "\n",
    "sample_size = 1000\n",
    "blogs_df_sample = blogs_df.sample(n=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = blogs_df_sample['Title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = []\n",
    "for title in titles:\n",
    "    sentence = title.lower()\n",
    "    sentence = re.sub(r'[^a-zA-Z0-9_-áéíóúñ ]', r'', sentence)  #remove extraneous characters (maybe a different encoding)\n",
    "    lemmatized.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create phrase (sentence) embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are used to query the endpoint and parse the response\n",
    "\n",
    "def query(model_predictor, text):\n",
    "    \"\"\"Query the model predictor.\"\"\"\n",
    "\n",
    "    encoded_text = json.dumps(text).encode(\"utf-8\")\n",
    "\n",
    "    query_response = model_predictor.predict(\n",
    "        encoded_text,\n",
    "        {\n",
    "            \"ContentType\": \"application/x-text\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        },\n",
    "    )\n",
    "    return query_response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    \"\"\"Parse response and return the embedding.\"\"\"\n",
    "\n",
    "    model_predictions = json.loads(query_response)\n",
    "    embedding = model_predictions[\"embedding\"]\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the selected model to an endpoint for real time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model_predictor = embedding_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the deployed model to generate the embeddings for each of the titles in our sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_predictor = Predictor('jumpstart-example-infer-tensorflow-tcem-2023-01-19-23-23-44-619')  #Specifiy endpoint name in case you wanna use an already deployed endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentence_vectors = [parse_response(query(model_predictor, title)) for title in lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_titles_df = pd.DataFrame(sentence_vectors)\n",
    "encoded_titles_df['blog_title_lemmatized'] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_titles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster phrases (sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral clsutering is a clustering algorithm based on graph theory. Spectral clustering uses information from the eigenvalues (spectrum) of the Laplacian matrix built from the graph or the data set to create groups (clusters) of data. Spectral clustering requires a measures of affinity between data points, for this application we use cosine affinity because we are interested in sentences that lie near to each other but also with \"similar meaning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 20\n",
    "\n",
    "clustering_model = SpectralClustering(n_clusters=n_clusters, n_init=100, affinity='cosine', n_neighbors=10, assign_labels=\"kmeans\", random_state=0)\n",
    "embeddings = encoded_titles_df[encoded_titles_df.columns[0:-1]]\n",
    "encoded_titles_df['cluster'] = clustering_model.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_titles = encoded_titles_df[['blog_title_lemmatized', 'cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic cluster labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TF-IDF for finding the keywords in each of our clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Frequency - Inverse Document Frequency is an NLP technique used to find the most relevant terms in set of documents (phrases in our case). From each cluster we extract its most relevant terms (nouns only) according to TF-IDF and use those as labels/categories for that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [clusters_titles.loc[clusters_titles.cluster == i, 'blog_title_lemmatized'].to_list() for i in range(0,n_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_tf_idf = []\n",
    "clusters_tf_idf_terms = []\n",
    "clusters_tags = []\n",
    "clusters_keywords_tf_idf = []\n",
    "tf_idf_threshold = 0.2\n",
    "\n",
    "for cluster in clusters:\n",
    "\n",
    "    tfIdfVectorizer = TfidfVectorizer(use_idf=True)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform(cluster)\n",
    "    tf_idf_df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    tf_idf_df = tf_idf_df.sort_values('TF-IDF', ascending=False)\n",
    "    \n",
    "    clusters_tf_idf.append(tf_idf_df)\n",
    "    \n",
    "    cluster_tf_idf_terms = list(tf_idf_df.loc[tf_idf_df['TF-IDF'] > tf_idf_threshold].index.values)\n",
    "    clusters_tf_idf_terms.append(cluster_tf_idf_terms)\n",
    "    \n",
    "    tags = nltk.pos_tag(cluster_tf_idf_terms)\n",
    "    clusters_tags.append(tags)\n",
    "    \n",
    "    keywords = [tag[0] for tag in tags if tag[1] in ['NN', 'NNS'] and tag[0] not in ['aws', 'amazon']]\n",
    "    clusters_keywords_tf_idf.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_idf = []\n",
    "\n",
    "for cluster in clusters:\n",
    "\n",
    "    cv = CountVectorizer() \n",
    "    word_count_vector = cv.fit_transform(cluster)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "    df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    "    df_idf['word'] = cv.get_feature_names()\n",
    "\n",
    "    df_idf = df_idf.sort_values(by='idf_weights')\n",
    "    clusters_idf.append(df_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_keywords_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_idf[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_keywords_idf = []\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "clusters_idf_words = [cluster['word'].to_list()[0:10] for cluster in clusters_idf]\n",
    "s = [ word for word in stop_words if word != 're'] #Remove stopwords but the word re (for re: invent)\n",
    "\n",
    "for cluster in clusters_idf_words:\n",
    "    tags = nltk.pos_tag(cluster)\n",
    "    words = [ tag[0] for tag in tags if tag[1] in ['NN', 'NNS'] and tag[0] not in ['aws', 'amazon']]\n",
    "    \n",
    "    clusters_keywords_idf.append(\",\".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_keywords_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch process the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will create batch processing jobs to process the entire dataset (roughly 24K titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import jsonlines\n",
    "\n",
    "from sagemaker.s3 import S3Downloader,S3Uploader,s3_path_join\n",
    "\n",
    "n_clusters = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = '<REPLACE_WITH_YOUR_BUCKET_NAME>'\n",
    "s3_prefix = '<REPLACE_WITH_YOUR_PREFIX>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chose a model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We choose the tensorflow-tcembedding-universal-sentence-encoder-cmlm-en-large-1 as the default model since it is better suited for phrase analysis\n",
    "\n",
    "model_id, model_version = (\n",
    "    \"tensorflow-tcembedding-universal-sentence-encoder-cmlm-en-large-1\", \n",
    "    \"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue with the default model, or can choose a different model from the dropdown generated upon running the next cell. A complete list of SageMaker pre-trained models can also be accessed at [Sagemaker pre-trained Models](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model from the selected model_id, model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = name_from_base(f\"jumpstart-example-infer-gpu-{model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "batch_transform_instance_type = \"ml.g4dn.xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base Tensorflow container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=batch_transform_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the inference script uri. This includes all dependencies and scripts for model loading, inference handling etc.\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the model uri. This includes the model and model parameters.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create the SageMaker model instance\n",
    "batch_transform_embedding_model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    model_data=model_uri,\n",
    "    entry_point=\"inference.py\",  # entry point file in source_dir and present in deploy_source_uri\n",
    "    role=sagemaker_execution_role,\n",
    "    name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs_df = pd.read_csv('aws_blog_titles.csv', header=None, names=['URL', 'Title'])\n",
    "aws_acronyms_df = pd.read_csv('acronyms.csv', header=None, delimiter=';', names=['acronym', 'meaning'])\n",
    "blogs_df = blogs_df.drop_duplicates(subset=['Title']).reset_index()\n",
    "blogs_df = blogs_df.drop(columns=['index', 'URL'])\n",
    "titles = blogs_df['Title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = []\n",
    "for title in titles:\n",
    "    sentence = title.lower()\n",
    "    sentence = re.sub(r'[^a-zA-Z0-9_-áéíóúñ ]', r'', sentence)  #remove extraneous characters (maybe a different encoding)\n",
    "    lemmatized.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the pre-processed data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_filename = 'aws_blog_titles.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(batch_filename, \"wb\") as txt_file:\n",
    "    for title in lemmatized:\n",
    "        \n",
    "        txt_file.write(json.dumps(title).encode(\"utf-8\"))\n",
    "        txt_file.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_upload_path = s3_path_join(\"s3://\",bucket_name,s3_prefix, 'raw')\n",
    "print(f\"Uploading data to {data_upload_path}\")\n",
    "data_uri = S3Uploader.upload(batch_filename, data_upload_path)\n",
    "print(f\"Uploaded data to {data_upload_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transformer to run a batch job\n",
    "\n",
    "output_path = s3_path_join(\"s3://\", bucket_name, s3_prefix, \"results\", \"embeddings\")\n",
    "\n",
    "batch_job = batch_transform_embedding_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=batch_transform_instance_type,\n",
    "    strategy='SingleRecord',\n",
    "    assemble_with='Line',\n",
    "    output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts batch transform job and uses S3 data as input. Enable the logs and wait only if you pass a small number of samples (< 100).\n",
    "# You can monitor your batch processing job from the SageMaker Console -> Inference -> Batch transform jobs\n",
    "batch_job.transform(\n",
    "    data=data_upload_path,\n",
    "    content_type='application/x-text',    \n",
    "    split_type='Line',\n",
    "    logs=False,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the results. \n",
    "#The batch transformation job (step above) must have finished before you can run this cell.\n",
    "embedding_data_path = s3_path_join(\"s3://\", bucket_name, s3_prefix, \"results\", \"embeddings\", batch_filename+'.out')\n",
    "print(f\"Downloading embeddings to .\")\n",
    "S3Downloader.download(embedding_data_path,'.')\n",
    "print(f\"Downloaded embeddings to .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "with jsonlines.open(batch_filename+\".out\", mode='r') as reader:\n",
    "    for obj in reader:\n",
    "        lines.append(obj['embedding'])\n",
    "        \n",
    "results_df = pd.DataFrame(lines)\n",
    "results_df['blog_title_lemmatized'] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_filename = \"blog_title_embeddings.csv\"\n",
    "results_df.to_csv(embeddings_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_upload_path = s3_path_join(\"s3://\",bucket_name,s3_prefix, 'embeddings')\n",
    "print(f\"Uploading embeddings to {embedding_upload_path}\")\n",
    "data_uri = S3Uploader.upload(embeddings_filename, embedding_upload_path)\n",
    "print(f\"Uploaded embeddings to {embedding_upload_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor_spectral_clustering = SKLearnProcessor(framework_version='1.0-1',\n",
    "                                                         role=sagemaker_execution_role,\n",
    "                                                         instance_type='ml.m5.2xlarge',\n",
    "                                                         instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_destination = os.path.join('s3://', bucket_name, s3_prefix, \"results\", \"clusters\")\n",
    "\n",
    "sklearn_processor_spectral_clustering.run(\n",
    "    code=\"./scikit-sagemaker-clustering/SpectralClustering.py\",\n",
    "    inputs=[ProcessingInput(source=embedding_upload_path, destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[ProcessingOutput(output_name=\"titles_clusters\", source=\"/opt/ml/processing/output\", destination=output_destination)],\n",
    "    arguments=[\"--n-clusters\", str(n_clusters),\n",
    "               \"--n-init\", \"100\",\n",
    "               \"--affinity\", \"cosine\",\n",
    "               \"--n-neighbors\", \"10\",\n",
    "               \"--assign-labels\", \"kmeans\"\n",
    "              ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the results. \n",
    "#The batch clustering job (step above) must have finished before you can run this cell.\n",
    "\n",
    "clusters_file = 'clustered_blog_titles_with_embeddings.csv'\n",
    "\n",
    "clusters_data_path = s3_path_join(\"s3://\", bucket_name, s3_prefix, \"results\", \"clusters\", clusters_file)\n",
    "print(f\"Downloading cluster data to .\")\n",
    "S3Downloader.download(clusters_data_path,'.')\n",
    "print(f\"Downloaded cluster data to .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic cluster labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df = pd.read_csv(clusters_file)\n",
    "clusters_titles = clusters_df[['blog_title_lemmatized', 'cluster_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [clusters_titles.loc[clusters_titles.cluster_label == i, 'blog_title_lemmatized'].to_list() for i in range(0, n_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_tf_idf = []\n",
    "clusters_tf_idf_terms = []\n",
    "clusters_tags = []\n",
    "clusters_keywords_tf_idf = []\n",
    "tf_idf_threshold = 0.2\n",
    "\n",
    "for cluster in clusters:\n",
    "\n",
    "    tfIdfVectorizer = TfidfVectorizer(use_idf=True)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform(cluster)\n",
    "    tf_idf_df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    tf_idf_df = tf_idf_df.sort_values('TF-IDF', ascending=False)\n",
    "    \n",
    "    clusters_tf_idf.append(tf_idf_df)\n",
    "    \n",
    "    cluster_tf_idf_terms = list(tf_idf_df.loc[tf_idf_df['TF-IDF'] > tf_idf_threshold].index.values)\n",
    "    clusters_tf_idf_terms.append(cluster_tf_idf_terms)\n",
    "    \n",
    "    tags = nltk.pos_tag(cluster_tf_idf_terms)\n",
    "    clusters_tags.append(tags)\n",
    "    \n",
    "    keywords = [tag[0] for tag in tags if tag[1] in ['NN', 'NNS'] and tag[0] not in ['aws', 'amazon']]\n",
    "    clusters_keywords_tf_idf.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_keywords_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df['categories'] = clusters_titles['cluster_label'].map(lambda i: clusters_keywords_tf_idf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df.loc[clusters_df['cluster_label']==0, ['blog_title_lemmatized', 'cluster_label', 'categories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_categories_file = 'aws_blog_titles_clusters_categories.csv'\n",
    "clusters_df.to_csv(clusters_categories_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_data_path = s3_path_join(\"s3://\", bucket_name, s3_prefix, \"results\", \"clusters\")\n",
    "print(f\"Uploading clusters to {clusters_data_path}\")\n",
    "clusters_file_uri = S3Uploader.upload(clusters_categories_file, clusters_data_path)\n",
    "print(f\"Uploaded clusters to {clusters_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sample_df = clusters_df.sample(n=1000).reset_index()\n",
    "title_embeddings_sample = cluster_sample_df.iloc[:,:-3]\n",
    "clusters_titles_sample = cluster_sample_df[['blog_title_lemmatized', 'cluster_label', 'categories']]\n",
    "clusters_titles_sample['short_categories'] = clusters_titles_sample['categories'].map(lambda x: x[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_titles_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_tsne = TSNE(perplexity=13, n_components=2, init='pca', n_iter=5000)\n",
    "tsne_embeddings = clusters_tsne.fit_transform(title_embeddings_sample)\n",
    "tsne_embeddings_df = pd.DataFrame(tsne_embeddings, columns=['x', 'y'])\n",
    "tsne_embeddings_df['cluster'] = clusters_titles_sample['cluster_label']\n",
    "tsne_embeddings_df['labels'] = clusters_titles_sample['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsne_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=[\n",
    "    '#efaf50',\n",
    "    '#a09934',\n",
    "    '#e31ad9',\n",
    "    '#cfcbb0',\n",
    "    '#1224c9',\n",
    "    '#669fa4',\n",
    "    '#087274',\n",
    "    '#787168',\n",
    "    '#3e93cb',\n",
    "    '#722823',\n",
    "    '#c8784c',\n",
    "    '#74ac48',\n",
    "    '#c31033',\n",
    "    '#5acc21',\n",
    "    '#2ef8ba',\n",
    "    '#c67ebe',\n",
    "    '#805004',\n",
    "    '#a8f43b',\n",
    "    '#442d6d',\n",
    "    '#9141ea',\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "ax = sns.scatterplot(data=tsne_embeddings_df, x='x', y='y', hue='cluster', legend='full', palette=colors, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_embeddings_df[['cluster', 'labels']].drop_duplicates('cluster').sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (main, Sep 15 2022, 01:51:29) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
